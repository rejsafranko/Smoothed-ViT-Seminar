{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## constants.py"
      ],
      "metadata": {
        "id": "dKXGmO_kx8ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Data Augmentation defaults\n",
        "TRAIN_TRANSFORMS = transforms.Compose([\n",
        "            transforms.Resize(32),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "TEST_TRANSFORMS = transforms.Compose([\n",
        "        transforms.Resize(32),\n",
        "        transforms.ToTensor()\n",
        "    ])"
      ],
      "metadata": {
        "id": "07Um2fHOx_iO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine_tunify.py"
      ],
      "metadata": {
        "id": "EvzRg8wQyGZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install robustness"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62dk2WKY0KS3",
        "outputId": "e94bf63d-966d-4959-8d22-2a01c7272557"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting robustness\n",
            "  Downloading robustness-1.2.1.post2-py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from robustness) (5.4.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from robustness) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from robustness) (1.21.6)\n",
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Collecting cox\n",
            "  Downloading cox-0.1.post3-py3-none-any.whl (18 kB)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from robustness) (1.7.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from robustness) (0.3.6)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from robustness) (1.51.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from robustness) (1.0.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from robustness) (0.11.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from robustness) (0.14.0+cu116)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.8/dist-packages (from robustness) (3.7.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from robustness) (3.2.2)\n",
            "Collecting py3nvml\n",
            "  Downloading py3nvml-0.2.7-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from robustness) (1.3.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from robustness) (1.13.0+cu116)\n",
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 12.7 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->robustness) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->robustness) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->robustness) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->robustness) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->robustness) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->robustness) (2022.6)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->robustness) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->robustness) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tables->robustness) (21.3)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.8/dist-packages (from tables->robustness) (2.8.4)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX->robustness) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->robustness) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->robustness) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->robustness) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->robustness) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->robustness) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->robustness) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->robustness) (3.0.4)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=22376870f57f081ffc435b0ce296fbd20c8b8c2f38fd0acec47011b57d774f57\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/03/bb/7a97840eb54479b328672e15a536e49dc60da200fb21564d53\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: smmap, xmltodict, gitdb, py3nvml, gitpython, tensorboardX, GPUtil, cox, robustness\n",
            "Successfully installed GPUtil-1.4.0 cox-0.1.post3 gitdb-4.0.10 gitpython-3.1.30 py3nvml-0.2.7 robustness-1.2.1.post2 smmap-5.0.0 tensorboardX-2.5.1 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from robustness.tools.custom_modules import SequentialWithArgs\n",
        "\n",
        "def ft(model_name, model_ft, num_classes, additional_hidden=0):\n",
        "    if model_name in [\"resnet\", \"resnet18\", \"resnet50\", \"wide_resnet50_2\", \"wide_resnet50_4\", \"wide_resnet101_2\", \"resnext50_32x4d\", 'shufflenet']:\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        # The two cases are split just to allow loading\n",
        "        # models trained prior to adding the additional_hidden argument\n",
        "        # without errors\n",
        "        if additional_hidden == 0:\n",
        "            model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        else:\n",
        "            model_ft.fc = SequentialWithArgs(\n",
        "                *list(sum([[nn.Linear(num_ftrs, num_ftrs), nn.ReLU()] for i in range(additional_hidden)], [])),\n",
        "                nn.Linear(num_ftrs, num_classes)\n",
        "            )\n",
        "        input_size = 224\n",
        "    elif 'regnet' in model_name:\n",
        "        num_ftrs = model_ft.head.fc.in_features\n",
        "        model_ft.head.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    elif 'deit' in model_name:\n",
        "        num_ftrs = model_ft.head.in_features\n",
        "        model_ft.head = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224 \n",
        "    elif model_name == \"alexnet\":\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "    elif \"vgg\" in model_name:\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "    elif model_name == \"squeezenet\":\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "    elif model_name == \"densenet\":\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "    elif model_name in [\"mnasnet\", \"mobilenet\"]:\n",
        "        num_ftrs = model_ft.classifier[1].in_features\n",
        "        model_ft.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type, exiting...\")\n",
        "\n",
        "    return model_ft"
      ],
      "metadata": {
        "id": "yDS9vhT0yIay"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## solution.py"
      ],
      "metadata": {
        "id": "S2WMM-emyRu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as ch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import itertools\n",
        "import math\n",
        "\n",
        "def ablate(x, pos, k, total_pos, dim): \n",
        "    # x : input\n",
        "    # pos : starting position\n",
        "    # k : size of ablation\n",
        "    # total_pos : maximum position\n",
        "    # dim : height or width (2 or 3)\n",
        "    inp = ch.zeros_like(x)\n",
        "    mask = x.new_zeros(x.size(0), 1, x.size(2), x.size(3))\n",
        "    if pos + k > total_pos: \n",
        "        idx1 = [slice(None,None,None) if _ != dim else slice(pos,total_pos,None) for _ in range(4)]\n",
        "        idx2 = [slice(None,None,None) if _ != dim else slice(0,pos+k-total_pos,None) for _ in range(4)]\n",
        "        inp[idx1] = x[idx1]\n",
        "        inp[idx2] = x[idx2]\n",
        "        mask[idx1] = 1\n",
        "        mask[idx2] = 1\n",
        "    else: \n",
        "        idx = [slice(None,None,None) if _ != dim else slice(pos,pos+k,None) for _ in range(4)]\n",
        "        inp[idx] = x[idx]\n",
        "        mask[idx] = 1\n",
        "    return ch.cat([inp,mask],dim=1)\n",
        "\n",
        "def ablate2(x,block_pos,block_k,shape): \n",
        "    inp = ch.zeros_like(x)\n",
        "    mask = x.new_zeros(x.size(0), 1, x.size(2), x.size(3))\n",
        "\n",
        "    slices = []\n",
        "    for pos,k,total_pos in zip(block_pos,block_k,shape): \n",
        "        if pos + k > total_pos: \n",
        "            slices.append([slice(0,pos+k-total_pos,None), slice(pos,total_pos,None)])\n",
        "        else: \n",
        "            slices.append([slice(pos,pos+k,None)])\n",
        "\n",
        "    for si,sj in itertools.product(*slices): \n",
        "        idx = [slice(None,None,None),slice(None,None,None),si,sj]\n",
        "        inp[idx] = x[idx]\n",
        "        mask[idx] = 1\n",
        "\n",
        "    return ch.cat([inp,mask],dim=1)\n",
        "    \n",
        "\n",
        "class DerandomizedSmoother(nn.Module): \n",
        "    def __init__(self, column_model=None, row_model=None, block_size=(4,4), stride=(1,1), preprocess=None): \n",
        "        super(DerandomizedSmoother, self).__init__()\n",
        "        self.column_model = column_model\n",
        "        self.row_model = row_model\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "        self.preprocess = preprocess\n",
        "        \n",
        "    def forward(self, x, nclasses=10, threshold=None, return_mode=None): \n",
        "        # return_mode == 'differentiable', 'ablations', 'predictions'\n",
        "        nex, nch, h, w = x.size()\n",
        "        \n",
        "        predictions = x.new_zeros(nex, nclasses)\n",
        "        softmaxes = 0\n",
        "        ablations = []\n",
        "        for model, total_pos, k, s, dim in zip((self.row_model, self.column_model), \n",
        "                                               (h,w), \n",
        "                                               self.block_size, \n",
        "                                               self.stride, \n",
        "                                               (2,3)): \n",
        "            if model is not None: \n",
        "                for pos in range(0,total_pos,s): \n",
        "                    inp = ablate(x, pos, k, total_pos, dim)\n",
        "                    if self.preprocess is not None: \n",
        "                        inp = self.preprocess(inp)\n",
        "                    out = model(inp)\n",
        "                    if isinstance(out, tuple): \n",
        "                        out = out[0]\n",
        "                    out = F.softmax(out,dim=1)\n",
        "\n",
        "                    if return_mode == 'differentiable': \n",
        "                        softmaxes += out\n",
        "                    if return_mode == 'ablations' or return_mode == 'all': \n",
        "                        ablations.append(out.max(1)[1].unsqueeze(1))\n",
        "\n",
        "                    if threshold is not None: \n",
        "                        predictions += (out >= threshold).int()\n",
        "                    else: \n",
        "                        predictions += (out.max(1)[0].unsqueeze(1) == out).int()\n",
        "        \n",
        "        if return_mode == 'differentiable': \n",
        "            return softmaxes/len(range(0,total_pos,s))\n",
        "        if return_mode == 'predictions': \n",
        "            return predictions.argmax(1), predictions\n",
        "        if return_mode == 'ablations': \n",
        "            return predictions.argmax(1), ch.cat(ablations,dim=1)\n",
        "        if return_mode == 'all': \n",
        "            return predictions.argmax(1), predictions, ch.cat(ablations,dim=1)\n",
        "\n",
        "        return predictions.argmax(1)\n",
        "\n",
        "class BlockDerandomizedSmoother(nn.Module): \n",
        "    def __init__(self, block_model=None, block_size=(4,4), stride=(1,1), preprocess=None): \n",
        "        super(BlockDerandomizedSmoother, self).__init__()\n",
        "        self.model = block_model\n",
        "        self.block_size = block_size\n",
        "        self.stride = stride\n",
        "        self.preprocess = preprocess\n",
        "        \n",
        "    def forward(self, x, nclasses=10, threshold=None, return_mode=None): \n",
        "        # return_mode == 'differentiable', 'ablations', 'predictions'\n",
        "        nex, nch, h, w = x.size()\n",
        "        \n",
        "        predictions = x.new_zeros(nex, nclasses)\n",
        "        softmaxes = 0\n",
        "        ablations = []\n",
        "\n",
        "        for i_pos in tqdm(range(0,h,self.stride[0])): \n",
        "            for j_pos in range(0,w,self.stride[1]): \n",
        "                inp = ablate2(x, (i_pos,j_pos), self.block_size, (h,w))\n",
        "                if self.preprocess is not None: \n",
        "                    inp = self.preprocess(inp)\n",
        "                out = self.model(inp)\n",
        "                if isinstance(out, tuple): \n",
        "                    out = out[0]\n",
        "                out = F.softmax(out,dim=1)\n",
        "\n",
        "                if return_mode == 'differentiable': \n",
        "                    softmaxes += out\n",
        "                if return_mode == 'ablations' or return_mode == 'all': \n",
        "                    ablations.append(out.max(1)[1].unsqueeze(1))\n",
        "\n",
        "                if threshold is not None: \n",
        "                    predictions += (out >= threshold).int()\n",
        "                else: \n",
        "                    predictions += (out.max(1)[0].unsqueeze(1) == out).int()\n",
        "        \n",
        "        if return_mode == 'differentiable': \n",
        "            return softmaxes/len(range(0,total_pos,s))\n",
        "        if return_mode == 'predictions': \n",
        "            return predictions.argmax(1), predictions\n",
        "        if return_mode == 'ablations': \n",
        "            return predictions.argmax(1), ch.cat(ablations,dim=1)\n",
        "        if return_mode == 'all': \n",
        "            return predictions.argmax(1), predictions, ch.cat(ablations,dim=1)\n",
        "\n",
        "\n",
        "        return predictions.argmax(1)\n",
        "\n",
        "def certify(args, model, validation_loader, store=None): \n",
        "    # print(\"Certification is replacing transform with ToTensor\")\n",
        "    m = args.certify_patch_size\n",
        "    s = args.certify_ablation_size\n",
        "    stride = args.certify_stride\n",
        "\n",
        "    if args.dataset == 'cifar10': \n",
        "        nclasses = 10\n",
        "    elif args.dataset == 'imagenet': \n",
        "        nclasses = 1000\n",
        "    else: \n",
        "        raise ValueError(\"Unknown number of classes\")\n",
        "\n",
        "    os.makedirs(args.certify_out_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(args.certify_out_dir, args.exp_name), exist_ok=True)\n",
        "    summary_path = os.path.join(args.certify_out_dir,args.exp_name,f\"m{m}_s{s}_summary.pth\")\n",
        "    if os.path.exists(summary_path): \n",
        "        d = ch.load(summary_path)\n",
        "        print(\"summary:\")\n",
        "        print(f\"acc: {d['acc']:.4f}, abl {d['ablation_acc']:.4f},  cert {d['cert_acc']:.4f}, delta: {d['delta']:.4f}, s: {s}, m: {m}\")\n",
        "        return d['delta']\n",
        "\n",
        "    model.eval() \n",
        "    model = nn.DataParallel(model)\n",
        "    with ch.no_grad(): \n",
        "        col_model = model if args.certify_mode in ['both', 'col'] else None\n",
        "        row_model = model if args.certify_mode in ['both', 'row'] else None\n",
        "\n",
        "        # number of ablations in one axis\n",
        "        na = math.ceil((m + s - 1)/stride)\n",
        "        if args.certify_mode == 'block': \n",
        "            smoothed_model = BlockDerandomizedSmoother(\n",
        "                block_model=model, \n",
        "                block_size=(s,s), \n",
        "                stride=(stride,stride)\n",
        "            )\n",
        "            gap = 2*(na**2) + 1\n",
        "        else: \n",
        "            smoothed_model = DerandomizedSmoother(\n",
        "                column_model=col_model, \n",
        "                row_model=row_model, \n",
        "                block_size=(s,s), \n",
        "                stride=(stride,stride)\n",
        "            )\n",
        "            # add one to not handle ties\n",
        "            # 2*(m + s - 1) for one dimension of ablations, and \n",
        "            # double again for two axes\n",
        "            factor = 4 if args.certify_mode == 'both' else 2\n",
        "            gap = na*factor + 1 \n",
        "\n",
        "        total = 0\n",
        "        n = 0\n",
        "        smooth_total = 0\n",
        "        certified_total = 0\n",
        "        ablation_total = 0\n",
        "        delta = 0\n",
        "        \n",
        "\n",
        "        pbar = tqdm(validation_loader)\n",
        "        for i,(X,y) in enumerate(pbar): \n",
        "            if args.batch_id != None and args.batch_id < i: \n",
        "                break\n",
        "            if args.batch_id != None and args.batch_id != i: \n",
        "                continue\n",
        "            file_path = os.path.join(args.certify_out_dir,args.exp_name,f\"m{m}_s{s}_b{i}.pth\")\n",
        "            if os.path.exists(file_path): \n",
        "                d = ch.load(file_path)\n",
        "\n",
        "                smooth_total += d['smooth_delta']\n",
        "                ablation_total += d['ablation_delta']\n",
        "                certified_total += d['certified_delta']\n",
        "                total += d['total_delta']\n",
        "                delta += d['delta_delta']\n",
        "                n += X.size(0)\n",
        "\n",
        "                pbar.set_description(f\"Acc: {total/n:.4f} Abl acc: {ablation_total/n:.4f} Smo acc: {smooth_total/n:.4f} Cer acc: {certified_total/n:.4f} Delta: {delta/n:.0f}\")\n",
        "                continue\n",
        "            X,y = X.cuda(),y.cuda()\n",
        "            acc = (model(X)[0].max(1)[1] == y).float().mean()\n",
        "\n",
        "            y_smoothed, y_counts, y_ablations = smoothed_model(X, return_mode=\"all\", nclasses=nclasses)\n",
        "            y_1st_vals, y_1st_idx = y_counts.kthvalue(nclasses,dim=1)\n",
        "            y_2nd_vals, y_2nd_idx = y_counts.kthvalue(nclasses-1,dim=1)\n",
        "\n",
        "            y_tar_vals = ch.gather(y_counts,1,y.unsqueeze(1)).squeeze()\n",
        "            not_y = (y_1st_idx != y)\n",
        "            y_nex_idx = y_1st_idx*(not_y.int()) + y_2nd_idx*(~not_y)\n",
        "            y_nex_vals = ch.gather(y_counts,1,y_nex_idx.unsqueeze(1)).squeeze()\n",
        "            \n",
        "            y_certified = (y == y_1st_idx)*(y_1st_vals >= y_2nd_vals + gap)\n",
        "\n",
        "            smooth_delta = (y_smoothed == y).sum().item()\n",
        "            smooth_total += smooth_delta\n",
        "\n",
        "            ablation_delta = y_tar_vals.sum().item()\n",
        "            ablation_total += ablation_delta\n",
        "\n",
        "            certified_delta = y_certified.sum().item()\n",
        "            certified_total += certified_delta\n",
        "\n",
        "            total_delta = acc.item()*X.size(0)\n",
        "            total += total_delta\n",
        "\n",
        "            delta_delta = (y_tar_vals - y_nex_vals).sum().item()\n",
        "            delta += delta_delta\n",
        "            n += X.size(0)\n",
        "\n",
        "            ch.save({\n",
        "                \"total_delta\" : total_delta, \n",
        "                \"certified_delta\" : certified_delta, \n",
        "                \"smooth_delta\": smooth_delta, \n",
        "                \"ablation_delta\": ablation_delta, \n",
        "                \"delta_delta\": delta_delta, \n",
        "                \"s\": s, \n",
        "                \"m\": m, \n",
        "                \"mode\": args.certify_mode, \n",
        "                \"ablations\": y_ablations.detach().cpu(), \n",
        "                \"y\": y.cpu()\n",
        "            }, file_path)\n",
        "\n",
        "            pbar.set_description(f\"Acc: {total/n:.4f} Abl acc: {ablation_total/n:.4f} Smo acc: {smooth_total/n:.4f} Cer acc: {certified_total/n:.4f} Delta: {delta/n:.0f}\")\n",
        "\n",
        "        if args.batch_id == None: \n",
        "            ch.save({\n",
        "                \"acc\" : total/n, \n",
        "                \"cert_acc\" : certified_total/n, \n",
        "                \"smooth_acc\": smooth_total/n, \n",
        "                \"ablation_acc\": ablation_total/n, \n",
        "                \"delta\": delta/n, \n",
        "                \"s\": s, \n",
        "                \"m\": m, \n",
        "                \"mode\": args.certify_mode\n",
        "            }, summary_path)\n",
        "\n",
        "            print(f\"acc: {total/n:.4f}, ablation {ablation_total/n:.4f}, smoothed {smooth_total/n:.4f}, certified {certified_total/n:.4f}, delta: {delta/n:.4f}, s: {s}, m: {m}\")\n",
        "        return delta/n"
      ],
      "metadata": {
        "id": "9t4bNM2tyVwc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vision_transformer.py"
      ],
      "metadata": {
        "id": "X0MOL6Qpywri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Vision Transformer (ViT) in PyTorch\n",
        "A PyTorch implement of Vision Transformers as described in\n",
        "'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale' - https://arxiv.org/abs/2010.11929\n",
        "The official jax code is released and available at https://github.com/google-research/vision_transformer\n",
        "Status/TODO:\n",
        "* Models updated to be compatible with official impl. Args added to support backward compat for old PyTorch weights.\n",
        "* Weights ported from official jax impl for 384x384 base and small models, 16x16 and 32x32 patches.\n",
        "* Trained (supervised on ImageNet-1k) my custom 'small' patch model to 77.9, 'base' to 79.4 top-1 with this code.\n",
        "* Hopefully find time and GPUs for SSL or unsupervised pretraining on OpenImages w/ ImageNet fine-tune in future.\n",
        "Acknowledgments:\n",
        "* The paper authors for releasing code and weights, thanks!\n",
        "* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\n",
        "for some einops/einsum fun\n",
        "* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n",
        "* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n",
        "Hacked together by / Copyright 2020 Ross Wightman\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "\n",
        "# from .resnet import resnet26d, resnet50d\n",
        "from enum import Enum\n",
        "\n",
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "\n",
        "class Debug_Flags(Enum):\n",
        "    L2_HEAD_CTX_NORMS = 1 # return the l2 norms of the context heads\n",
        "    ATTN_MASKS = 2 # return the attention masks\n",
        "    HEAD_CTX = 3 # return the head contexts\n",
        "    LAYER_CTX = 4 # return the aggregated head contexts\n",
        "    HEAD_OUTPUT = 5 # return the attention output from each head\n",
        "    LAYER_OUTPUT = 6 # return the output from aggregated heads\n",
        "    RESIDUAL_CTX_VEC = 7 # residual before adding layer output\n",
        "    RESIDUAL_CTX_ADD = 8 # ctx right before adding to residual\n",
        "    RESIDUAL_CTX_FINAL = 9 # RESIDUAL_VEC + RESIDUAL_ADD\n",
        "    FINAL_LATENT_VECTOR = 10 # final latent vector for the model\n",
        "    PATCH_EMBED = 11\n",
        "    RESIDUAL_LAYER_FINAL = 12 # RESIDUAL_VEC + RESIDUAL_ADD but for the whole raster\n",
        "    NO_POS = 13\n",
        "\n",
        "\n",
        "def _cfg(url='', **kwargs):\n",
        "    return {\n",
        "        'url': url,\n",
        "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
        "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
        "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
        "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
        "        **kwargs\n",
        "    }\n",
        "\n",
        "\n",
        "default_cfgs = {\n",
        "    # patch models\n",
        "    'vit_small_patch16_224': _cfg(\n",
        "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth',\n",
        "    ),\n",
        "    'vit_base_patch16_224': _cfg(\n",
        "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',\n",
        "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
        "    ),\n",
        "    'vit_base_patch16_384': _cfg(\n",
        "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',\n",
        "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
        "    'vit_base_patch32_384': _cfg(\n",
        "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p32_384-830016f5.pth',\n",
        "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
        "    'vit_large_patch16_224': _cfg(\n",
        "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_224-4ee7a4dc.pth',\n",
        "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    'vit_large_patch16_384': _cfg(\n",
        "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_384-b3be5167.pth',\n",
        "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
        "    'vit_large_patch32_384': _cfg(\n",
        "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n",
        "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
        "    'vit_huge_patch16_224': _cfg(),\n",
        "    'vit_huge_patch32_384': _cfg(input_size=(3, 384, 384)),\n",
        "    # hybrid models\n",
        "    'vit_small_resnet26d_224': _cfg(),\n",
        "    'vit_small_resnet50d_s3_224': _cfg(),\n",
        "    'vit_base_resnet26d_224': _cfg(),\n",
        "    'vit_base_resnet50d_224': _cfg(),\n",
        "    # deit models\n",
        "    'deit_tiny_patch16_224': _cfg(\n",
        "        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth',\n",
        "        crop_pct=0.875, interpolation=3\n",
        "    ),\n",
        "    'deit_small_patch16_224': _cfg(\n",
        "        url='https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth',\n",
        "        crop_pct=0.875, interpolation=3\n",
        "    ),\n",
        "    'deit_base_patch16_224': _cfg(\n",
        "        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth',\n",
        "        crop_pct=0.875, interpolation=3\n",
        "    ),\n",
        "    'deit_base_patch16_384': _cfg(\n",
        "        crop_pct=1.0, interpolation=4,  input_size=(3, 384, 384),\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x, head_hide_list=None, debug_flags=None):\n",
        "        debug = {}\n",
        "        if debug_flags is None:\n",
        "            debug_flags = {}\n",
        "\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = (attn - torch.max(attn, dim=-1, keepdim=True)[0]).softmax(dim=-1)\n",
        "        if head_hide_list is not None:\n",
        "            # we are masking some heads\n",
        "            if -1 in head_hide_list:\n",
        "                # mask all in this layer\n",
        "                attn[:, :, :, 1:] *= 0\n",
        "                attn[:, :, :, 0] = 1\n",
        "            else:\n",
        "                attn[:, head_hide_list, :, 1:] *= 0\n",
        "                attn[:, head_hide_list, :, 0] = 1\n",
        "\n",
        "        if Debug_Flags.ATTN_MASKS in debug_flags:\n",
        "            debug[Debug_Flags.ATTN_MASKS.name] = attn # store attention mask\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2) # B, N, Num Heads, head_dim\n",
        "        if Debug_Flags.L2_HEAD_CTX_NORMS in debug_flags:\n",
        "            l2_norms = x[:, 0].norm(dim=-1)\n",
        "            debug[Debug_Flags.L2_HEAD_CTX_NORMS.name] = l2_norms\n",
        "        if Debug_Flags.HEAD_CTX in debug_flags:\n",
        "            head_contexts = x[:, 0]\n",
        "            debug[Debug_Flags.HEAD_CTX.name] = head_contexts\n",
        "        if Debug_Flags.HEAD_OUTPUT in debug_flags:\n",
        "            head_outputs = x\n",
        "            debug[Debug_Flags.HEAD_OUTPUT.name] = head_outputs\n",
        "        \n",
        "        x = x.reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        if Debug_Flags.LAYER_CTX in debug_flags:\n",
        "            layer_contexts = x[:, 0]\n",
        "            debug[Debug_Flags.LAYER_CTX.name] = layer_contexts\n",
        "        if Debug_Flags.LAYER_OUTPUT in debug_flags:\n",
        "            layer_outputs = x\n",
        "            debug[Debug_Flags.LAYER_OUTPUT.name] = layer_outputs\n",
        "        x = self.proj_drop(x)\n",
        "        return x, debug\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def forward(self, x, head_hide_list=None, debug_flags=None):\n",
        "        if debug_flags is None:\n",
        "            debug_flags = {}\n",
        "        out, debug = self.attn(self.norm1(x), head_hide_list=head_hide_list, debug_flags=debug_flags)\n",
        "        out = self.drop_path(out)\n",
        "        if Debug_Flags.RESIDUAL_CTX_ADD in debug_flags:\n",
        "            debug[Debug_Flags.RESIDUAL_CTX_ADD.name] = out[:, 0]\n",
        "        if Debug_Flags.RESIDUAL_CTX_VEC in debug_flags:\n",
        "            debug[Debug_Flags.RESIDUAL_CTX_VEC.name] = x[:, 0]\n",
        "        x = x + out\n",
        "        if Debug_Flags.RESIDUAL_CTX_FINAL in debug_flags:\n",
        "            debug[Debug_Flags.RESIDUAL_CTX_FINAL.name] = x[:, 0]\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        if Debug_Flags.RESIDUAL_LAYER_FINAL in debug_flags:\n",
        "            debug[Debug_Flags.RESIDUAL_LAYER_FINAL.name] = x\n",
        "        return x, debug\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # FIXME look at relaxing size constraints\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HybridEmbed(nn.Module):\n",
        "    \"\"\" CNN Feature Map Embedding\n",
        "    Extract feature map from CNN, flatten, project to embedding dim.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        assert isinstance(backbone, nn.Module)\n",
        "        img_size = to_2tuple(img_size)\n",
        "        self.img_size = img_size\n",
        "        self.backbone = backbone\n",
        "        if feature_size is None:\n",
        "            with torch.no_grad():\n",
        "                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
        "                # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
        "                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
        "                training = backbone.training\n",
        "                if training:\n",
        "                    backbone.eval()\n",
        "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
        "                feature_size = o.shape[-2:]\n",
        "                feature_dim = o.shape[1]\n",
        "                backbone.train(training)\n",
        "        else:\n",
        "            feature_size = to_2tuple(feature_size)\n",
        "            feature_dim = self.backbone.feature_info.channels()[-1]\n",
        "        self.num_patches = feature_size[0] * feature_size[1]\n",
        "        self.proj = nn.Linear(feature_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)[-1]\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
        "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
        "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, norm_embed=False):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "\n",
        "        if hybrid_backbone is not None:\n",
        "            self.patch_embed = HybridEmbed(\n",
        "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        else:\n",
        "            self.patch_embed = PatchEmbed(\n",
        "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "\n",
        "        # self.norm_embed = norm_layer(embed_dim) if norm_embed else None\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n",
        "        #self.repr = nn.Linear(embed_dim, representation_size)\n",
        "        #self.repr_act = nn.Tanh()\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "        self.mask_processor = MaskProcessor(patch_size=patch_size)\n",
        "        self.first_pass = True\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed', 'cls_token'}\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        self.num_classes = num_classes\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward_features(self, x, head_mask, debug_flags, patch_mask): \n",
        "        if x.shape[1] == 4:\n",
        "            if self.first_pass:\n",
        "                print(\"USING MISSINGNESS\")\n",
        "            self.first_pass = False\n",
        "            assert patch_mask is None\n",
        "            x, ones_mask = x[:, :3], x[:, 3]\n",
        "            patch_mask = self.mask_processor(ones_mask)\n",
        "            \n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        patch_embedding = x\n",
        "        if head_mask is None:\n",
        "            head_mask = {}\n",
        "\n",
        "        # if self.norm_embed:\n",
        "        #     x = self.norm_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        if debug_flags is not None and Debug_Flags.NO_POS in debug_flags:\n",
        "            x = x\n",
        "        else:\n",
        "            x = x + self.pos_embed\n",
        "        x = self.pos_drop(x) # B, N, C\n",
        "        if patch_mask is not None:\n",
        "            # patch_mask is B, K\n",
        "            B, N, C = x.shape\n",
        "            if len(patch_mask.shape) == 1: # not a separate one per batch\n",
        "                x = x[:, patch_mask]\n",
        "            else:\n",
        "                patch_mask = patch_mask.unsqueeze(-1).expand(-1, -1, C)\n",
        "                x = torch.gather(x, 1, patch_mask)\n",
        "\n",
        "        all_debug = []\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            if i in head_mask:\n",
        "                x, all_debug_layer = blk(x, head_hide_list=head_mask[i], debug_flags=debug_flags)\n",
        "            else:\n",
        "                x, all_debug_layer = blk(x, debug_flags=debug_flags)\n",
        "            all_debug.append(all_debug_layer)\n",
        "\n",
        "        consolidated_all_debug = {}\n",
        "        for e in all_debug[0].keys():\n",
        "            consolidated_all_debug[e] = torch.stack([d[e] for d in all_debug], 1)\n",
        "        \n",
        "        if debug_flags is not None and Debug_Flags.PATCH_EMBED in debug_flags:\n",
        "            consolidated_all_debug[Debug_Flags.PATCH_EMBED.name] = patch_embedding\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return x[:, 0], consolidated_all_debug\n",
        "\n",
        "    def forward(self, x, head_mask=None, debug_flags=None, patch_mask=None):\n",
        "        # dict of layer_index -> list of head indices to turn off. If list just contains -1, turn all heads off in that layer\n",
        "        x, debug = self.forward_features(x, head_mask=head_mask, debug_flags=debug_flags, patch_mask=patch_mask)\n",
        "        if debug_flags is not None and Debug_Flags.FINAL_LATENT_VECTOR in debug_flags:\n",
        "            debug[Debug_Flags.FINAL_LATENT_VECTOR.name] = x\n",
        "        x = self.head(x)\n",
        "        if debug_flags is None:\n",
        "            return x\n",
        "        else:\n",
        "            return x, debug\n",
        "\n",
        "\n",
        "def _conv_filter(state_dict, patch_size=16):\n",
        "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
        "    out_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if 'patch_embed.proj.weight' in k:\n",
        "            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n",
        "        out_dict[k] = v\n",
        "    return out_dict\n",
        "\n",
        "\n",
        "def vit_small_patch16_224(pretrained=False, **kwargs):\n",
        "    if pretrained:\n",
        "        # NOTE my scale was wrong for original weights, leaving this here until I have better ones for this model\n",
        "        kwargs.setdefault('qk_scale', 768 ** -0.5)\n",
        "    model = VisionTransformer(patch_size=16, embed_dim=768, depth=8, num_heads=8, mlp_ratio=3., **kwargs)\n",
        "    model.default_cfg = default_cfgs['vit_small_patch16_224']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_base_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['vit_base_patch16_224']\n",
        "    if pretrained:\n",
        "        load_pretrained(\n",
        "            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_base_patch16_384(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['vit_base_patch16_384']\n",
        "    if pretrained:\n",
        "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_base_patch32_384(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=384, patch_size=32, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['vit_base_patch32_384']\n",
        "    if pretrained:\n",
        "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_large_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['vit_large_patch16_224']\n",
        "    if pretrained:\n",
        "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_large_patch16_384(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=384, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,  qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['vit_large_patch16_384']\n",
        "    if pretrained:\n",
        "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_large_patch32_384(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=384, patch_size=32, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4,  qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['vit_large_patch32_384']\n",
        "    if pretrained:\n",
        "        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_huge_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(patch_size=16, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, **kwargs)\n",
        "    model.default_cfg = default_cfgs['vit_huge_patch16_224']\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_huge_patch32_384(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=384, patch_size=32, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, **kwargs)\n",
        "    model.default_cfg = default_cfgs['vit_huge_patch32_384']\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_tiny_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['deit_tiny_patch16_224']\n",
        "    if pretrained:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n",
        "            map_location=\"cpu\", check_hash=True\n",
        "        )\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "        print('==>[Loaded PyTorch-pretrained deit checkpoint.]')\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_small_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['deit_small_patch16_224']\n",
        "    if pretrained:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n",
        "            map_location=\"cpu\", check_hash=True\n",
        "        )\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "        print('==>[Loaded PyTorch-pretrained deit checkpoint.]')\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_base_patch16_224(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['deit_base_patch16_224']\n",
        "    if pretrained:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n",
        "            map_location=\"cpu\", check_hash=True\n",
        "        )\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "        print('==>[Loaded PyTorch-pretrained deit checkpoint.]')\n",
        "    return model\n",
        "\n",
        "def deit_base_patch16_384(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    model.default_cfg = default_cfgs['deit_base_patch16_384']\n",
        "    if pretrained:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth\",\n",
        "            map_location=\"cpu\", check_hash=True\n",
        "        )\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "        print('==>[Loaded PyTorch-pretrained deit checkpoint.]')\n",
        "    return model\n",
        "\n",
        "\n",
        "### CIFAR 10 \n",
        "def deit_tiny_patch4_32(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=32, patch_size=4, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_small_patch4_32(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=32, patch_size=4, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def deit_base_patch4_32(pretrained=False, **kwargs):\n",
        "    model = VisionTransformer(\n",
        "        img_size=32, patch_size=4, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Le5fdGlQyyuO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transfer_utils.py"
      ],
      "metadata": {
        "id": "L0RtKodtyeDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "from robustness import datasets, model_utils\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision.datasets import CIFAR100\n",
        "import torch as ch\n",
        "\n",
        "\n",
        "pytorch_models = {\n",
        "    'alexnet': models.alexnet,\n",
        "    'vgg16': models.vgg16,\n",
        "    'vgg16_bn': models.vgg16_bn,\n",
        "    'squeezenet': models.squeezenet1_0,\n",
        "    'densenet': models.densenet161,\n",
        "    'shufflenet': models.shufflenet_v2_x1_0,\n",
        "    'mobilenet': models.mobilenet_v2,\n",
        "    'resnext50_32x4d': models.resnext50_32x4d,\n",
        "    'mnasnet': models.mnasnet1_0\n",
        "}\n",
        "\n",
        "vitmodeldict = {\n",
        "    # ImageNet\n",
        "    'deit_tiny_patch16_224': deit_tiny_patch16_224,\n",
        "    'deit_small_patch16_224': deit_small_patch16_224,\n",
        "    'deit_base_patch16_224': deit_base_patch16_224,\n",
        "    'deit_base_patch16_384': deit_base_patch16_384,\n",
        "    ##CIFAR10\n",
        "    'deit_tiny_patch4_32': deit_tiny_patch4_32,\n",
        "    'deit_small_patch4_32': deit_small_patch4_32,\n",
        "    'deit_base_patch4_32': deit_base_patch4_32,\n",
        "}\n",
        "\n",
        "TRANSFER_DATASETS = ['cifar10', 'cifar100']\n",
        "\n",
        "def get_dataset_and_loaders(args, shuffle_train=True, shuffle_val=False):\n",
        "    '''Given arguments, returns a datasets object and the train and validation loaders.\n",
        "    '''\n",
        "    ds = datasets.CIFAR(args.data)\n",
        "    ds.transform_train = TRAIN_TRANSFORMS\n",
        "    ds.transform_test = TEST_TRANSFORMS\n",
        "    img_size = 32\n",
        "\n",
        "    train_loader, val_loader = ds.make_loaders(only_val=args.eval_only, batch_size=args.batch_size, \n",
        "                                    workers=args.workers, shuffle_train=shuffle_train, shuffle_val=shuffle_val)\n",
        "    return ds, train_loader, val_loader\n",
        "\n",
        "\n",
        "def resume_finetuning_from_checkpoint(args, ds, finetuned_model_path):\n",
        "    '''Given arguments, dataset object and a finetuned model_path, returns a model\n",
        "    with loaded weights and returns the checkpoint necessary for resuming training.\n",
        "    '''\n",
        "    print('[Resuming finetuning from a checkpoint...]')\n",
        "    arch, add_custom_forward = get_arch(args)\n",
        "    if args.dataset in TRANSFER_DATASETS:\n",
        "        model, _ = model_utils.make_and_restore_model(\n",
        "            arch=arch, dataset=datasets.ImageNet(''), add_custom_forward=add_custom_forward)\n",
        "        while hasattr(model, 'model'):\n",
        "            model = model.model\n",
        "        model = ft(\n",
        "            args.arch, model, ds.num_classes, args.additional_hidden)\n",
        "        model, checkpoint = model_utils.make_and_restore_model(arch=model, dataset=ds, resume_path=finetuned_model_path,\n",
        "                                                               add_custom_forward=args.additional_hidden > 0 or add_custom_forward)\n",
        "    else:\n",
        "        model, checkpoint = model_utils.make_and_restore_model(\n",
        "            arch=arch, dataset=ds, resume_path=finetuned_model_path,\n",
        "            add_custom_forward=add_custom_forward)\n",
        "    return model, checkpoint\n",
        "\n",
        "\n",
        "def get_arch(args):\n",
        "    add_custom_forward = True\n",
        "    if args.arch in pytorch_models.keys():\n",
        "        arch = pytorch_models[args.arch](args.pytorch_pretrained)\n",
        "    elif args.arch in vitmodeldict:\n",
        "        arch = vitmodeldict[args.arch](pretrained=args.pytorch_pretrained,\n",
        "                                        num_classes=1000,\n",
        "                                        drop_rate=0.,\n",
        "                                        drop_path_rate=0.1)\n",
        "    else:\n",
        "        arch = args.arch\n",
        "        add_custom_forward = False\n",
        "    return arch, add_custom_forward\n",
        "\n",
        "def get_model(args, ds):\n",
        "    '''Given arguments and a dataset object, returns an ImageNet model (with appropriate last layer changes to \n",
        "    fit the target dataset) and a checkpoint. The checkpoint is set to None if not resuming training.\n",
        "    '''\n",
        "  \n",
        "    finetuned_model_path = os.path.join(\n",
        "        args.out_dir, args.exp_name, args.resume_ckpt_name)\n",
        "\n",
        "    if args.resume and os.path.isfile(finetuned_model_path):\n",
        "        # fix hijacking of normalizer\n",
        "        patch_state_dict(finetuned_model_path)\n",
        "        model, checkpoint = resume_finetuning_from_checkpoint(\n",
        "            args, ds, finetuned_model_path)\n",
        "    else:\n",
        "        arch, add_custom_forward = get_arch(args)\n",
        "        if args.dataset in TRANSFER_DATASETS:\n",
        "            model, _ = model_utils.make_and_restore_model(\n",
        "                arch=arch,\n",
        "                dataset=datasets.ImageNet(''), resume_path=args.model_path, pytorch_pretrained=args.pytorch_pretrained,\n",
        "                add_custom_forward=add_custom_forward)\n",
        "            checkpoint = None\n",
        "        else:\n",
        "            model, _ = model_utils.make_and_restore_model(arch=arch, dataset=ds,\n",
        "                                                resume_path=args.model_path, pytorch_pretrained=args.pytorch_pretrained,\n",
        "                                                add_custom_forward=add_custom_forward)\n",
        "            checkpoint = None\n",
        "\n",
        "        if not args.no_replace_last_layer and not args.eval_only and args.dataset in TRANSFER_DATASETS:\n",
        "            print(f'[Replacing the last layer with {args.additional_hidden} '\n",
        "                  f'hidden layers and 1 classification layer that fits the {args.dataset} dataset.]')\n",
        "            while hasattr(model, 'model'):\n",
        "                model = model.model\n",
        "            model = ft(\n",
        "                args.arch, model, ds.num_classes, args.additional_hidden)\n",
        "            model, checkpoint = model_utils.make_and_restore_model(arch=model, dataset=ds,\n",
        "                                                                   add_custom_forward=args.additional_hidden > 0 or add_custom_forward)\n",
        "        else:\n",
        "            print('[NOT replacing the last layer]')\n",
        "    return model, checkpoint\n",
        "\n",
        "\n",
        "def freeze_model(model, freeze_level):\n",
        "    '''\n",
        "    Freezes up to args.freeze_level layers of the model (assumes a resnet model)\n",
        "    '''\n",
        "    # Freeze layers according to args.freeze-level\n",
        "    update_params = None\n",
        "    if freeze_level != -1:\n",
        "        # assumes a resnet architecture\n",
        "        assert len([name for name, _ in list(model.named_parameters())\n",
        "                    if f\"layer{freeze_level}\" in name]), \"unknown freeze level (only {1,2,3,4} for ResNets)\"\n",
        "        update_params = []\n",
        "        freeze = True\n",
        "        for name, param in model.named_parameters():\n",
        "            print(name, param.size())\n",
        "\n",
        "            if not freeze and f'layer{freeze_level}' not in name:\n",
        "                print(f\"[Appending the params of {name} to the update list]\")\n",
        "                update_params.append(param)\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "\n",
        "            if freeze and f'layer{freeze_level}' in name:\n",
        "                # if the freeze level is detected stop freezing onwards\n",
        "                freeze = False\n",
        "    return update_params\n",
        "\n",
        "def patch_state_dict(path): \n",
        "    pth = torch.load(path)\n",
        "    d = pth['model']\n",
        "\n",
        "    if (\"normalizer.1.new_mean\" in d or \"normalizer.1.new_std\" in d\n",
        "        or \"module.normalizer.1.new_mean\" in d \n",
        "        or \"module.normalizer.1.new_std\" in d\n",
        "        or \"normalizer.normalizer.new_mean\" in d \n",
        "        or \"normalizer.normalizer.new_std\" in d\n",
        "        or \"module.normalizer.normalizer.new_mean\" in d \n",
        "        or \"module.normalizer.normalizer.new_std\" in d): \n",
        "        print(\"Patching normalizer module\")\n",
        "        new_d = {}\n",
        "        for k in d: \n",
        "            new_k = k\n",
        "            if k == \"normalizer.1.new_mean\": \n",
        "                new_k = \"normalizer.new_mean\"\n",
        "            if k == \"normalizer.1.new_std\": \n",
        "                new_k = \"normalizer.new_std\"\n",
        "            if k == \"module.normalizer.1.new_mean\": \n",
        "                new_k = \"module.normalizer.new_mean\"\n",
        "            if k == \"module.normalizer.1.new_std\": \n",
        "                new_k = \"module.normalizer.new_std\"\n",
        "            if k == \"normalizer.normalizer.new_mean\": \n",
        "                new_k = \"normalizer.new_mean\"\n",
        "            if k == \"normalizer.normalizer.new_std\": \n",
        "                new_k = \"normalizer.new_std\"\n",
        "            if k == \"module.normalizer.normalizer.new_mean\": \n",
        "                new_k = \"module.normalizer.new_mean\"\n",
        "            if k == \"module.normalizer.normalizer.new_std\": \n",
        "                new_k = \"module.normalizer.new_std\"\n",
        "            new_d[new_k] = d[k]\n",
        "        pth['model'] = new_d\n",
        "        torch.save(pth,path)\n",
        "    return"
      ],
      "metadata": {
        "id": "XGuL0zOjygTr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocess.py"
      ],
      "metadata": {
        "id": "MjTFta0fy6sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch as ch\n",
        "\n",
        "class StripeAblator(nn.Module):\n",
        "    def __init__(self, ablation_size, dim=3):\n",
        "        super().__init__()\n",
        "        self.ablation_size = ablation_size\n",
        "        self.dim = dim\n",
        "            \n",
        "    def forward(self, x, pos):\n",
        "        k = self.ablation_size\n",
        "        dim = self.dim\n",
        "        total_pos = x.shape[dim]\n",
        "        if pos + k > total_pos: \n",
        "            idx = [slice(None,None,None) if _ != dim else slice(pos+k-total_pos,pos,None) for _ in range(4)]\n",
        "            x[idx] = 0\n",
        "        else: \n",
        "            left_idx = [slice(None,None,None) if _ != dim else slice(0, pos, None) for _ in range(4)]\n",
        "            right_idx = [slice(None,None,None) if _ != dim else slice(pos+k, total_pos, None) for _ in range(4)]\n",
        "            x[left_idx] = 0\n",
        "            x[right_idx] = 0\n",
        "        return x\n",
        "\n",
        "class BlockAblator(nn.Module):\n",
        "    def __init__(self, ablation_size):\n",
        "        super().__init__()\n",
        "        self.ablation_size = ablation_size\n",
        "            \n",
        "    def forward(self, x, pos):\n",
        "        \"\"\"\n",
        "        x: input to be ablated\n",
        "        pos: tuple (idx_x, idx_y) representing the position of ablation to be applied\n",
        "        returns: ablated image\n",
        "        \"\"\"\n",
        "        assert len(pos) == 2\n",
        "\n",
        "        k = self.ablation_size\n",
        "        total_pos = x.shape[-1]\n",
        "        pos_x, pos_y = pos\n",
        "        x_orig = x.clone()\n",
        "        x[:, :, pos_x:(pos_x + k), pos_y:(pos_y + k)] = 0\n",
        "        if pos_x + k > total_pos and pos_y + k > total_pos:\n",
        "            x[:, :, 0:(pos_x + k)%total_pos, 0:(pos_y + k)%total_pos] = 0\n",
        "            x[:, :, 0:(pos_x + k)%total_pos, pos_y:(pos_y + k)] = 0\n",
        "            x[:, :, pos_x:(pos_x + k), 0:(pos_y + k)%total_pos] = 0\n",
        "        elif pos_x + k > total_pos:\n",
        "            x[:, :, 0:(pos_x + k)%total_pos, pos_y:(pos_y + k)] = 0\n",
        "        elif pos_y + k > total_pos:\n",
        "            x[:, :, pos_x:(pos_x + k), 0:(pos_y + k)%total_pos] = 0\n",
        "\n",
        "        return x_orig - x\n",
        "\n",
        "class Simple224Upsample(nn.Module):\n",
        "    # go from 32 to 224\n",
        "    def __init__(self, arch=''):\n",
        "        super(Simple224Upsample, self).__init__()\n",
        "        self.upsample = nn.Upsample(mode='nearest', scale_factor=7)\n",
        "        self.arch = arch\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.upsample(x)\n",
        "\n",
        "class Upsample384AndPad(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Upsample384AndPad, self).__init__()\n",
        "        self.upsample = nn.Upsample(mode='nearest', scale_factor=8) # 256\n",
        "        self.zero_pad = torch.nn.ZeroPad2d((384-256)//2) # 64 on each side\n",
        "    \n",
        "    def forward(self, x, ones_mask):\n",
        "        x = self.upsample(x)\n",
        "        x = self.zero_pad(x)\n",
        "        return x\n",
        "    \n",
        "cifar_upsamples = {\n",
        "    'simple224': Simple224Upsample,\n",
        "    'upsample384': Upsample384AndPad,\n",
        "    'none': None,\n",
        "}\n",
        "\n",
        "class MaskProcessor(nn.Module):\n",
        "    def __init__(self, patch_size=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AvgPool2d(patch_size)\n",
        "    \n",
        "    def forward(self, ones_mask):\n",
        "        B = ones_mask.shape[0]\n",
        "        ones_mask = ones_mask[0].unsqueeze(0) # take the first mask\n",
        "        ones_mask = self.avg_pool(ones_mask)[0]\n",
        "        ones_mask = torch.where(ones_mask.view(-1) > 0)[0] + 1\n",
        "        ones_mask = torch.cat([torch.cuda.IntTensor(1).fill_(0), ones_mask]).unsqueeze(0)\n",
        "        ones_mask = ones_mask.expand(B, -1)\n",
        "        return ones_mask\n",
        "    \n",
        "class PreProcessor(nn.Module):\n",
        "    def __init__(self, normalizer, ablation_size, upsample_type='none',\n",
        "                 return_mask=False, do_ablation=True, ablation_type='col', ablation_target=None):\n",
        "        '''\n",
        "        normalizer: the normalizer module\n",
        "        ablation_size: size of ablation\n",
        "        upsample_type: type of upsample (none, simple224, upsample384)\n",
        "        return_mask: if true, keep the mask as a fourth channel\n",
        "        do_ablation: perform the ablation\n",
        "        ablation_target: the column to ablate. if None, pick a random column\n",
        "        '''\n",
        "        super().__init__()\n",
        "        print({\n",
        "            \"ablation_size\": ablation_size,\n",
        "            \"upsample_type\": upsample_type,\n",
        "            \"return_mask\": return_mask,\n",
        "            \"do_ablation\": do_ablation,\n",
        "            \"ablation_target\": ablation_target\n",
        "        })\n",
        "        if ablation_type == 'col':\n",
        "            self.ablator = StripeAblator(ablation_size, dim=3)\n",
        "        elif ablation_type == 'block':\n",
        "            self.ablator = BlockAblator(ablation_size)\n",
        "        else:\n",
        "            raise Exception('Unkown ablation type')\n",
        "\n",
        "        if upsample_type == 'none':\n",
        "            self.upsampler = None\n",
        "        else:\n",
        "            self.upsampler = cifar_upsamples[upsample_type]()\n",
        "        self.return_mask = return_mask\n",
        "        self.normalizer = normalizer\n",
        "        self.do_ablation = do_ablation\n",
        "        self.ablation_target = ablation_target\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        if C == 3:\n",
        "            # we don't have a mask yet!!\n",
        "            ones = torch.ones((B, 1, H, W)).cuda()\n",
        "            x = torch.cat([x, ones], dim=1)\n",
        "        else:\n",
        "            assert not self.do_ablation, \"cannot do ablation if already passed in ablation mask\"\n",
        "        if self.do_ablation:\n",
        "            pos = self.ablation_target\n",
        "            if pos is None:\n",
        "                if isinstance(self.ablator, StripeAblator):\n",
        "                    pos = ch.randint(x.shape[3], (1,))\n",
        "                elif isinstance(self.ablator, BlockAblator):\n",
        "                    pos = ch.randint(x.shape[3], (2,))\n",
        "            x = self.ablator(x=x, pos=pos)\n",
        "        if self.upsampler is not None:\n",
        "            x = self.upsampler(x)\n",
        "        x[:, :3] = self.normalizer(x[:, :3]) # normalize\n",
        "        if self.return_mask:\n",
        "            return x # WARNING returning 4 channel output\n",
        "        else:\n",
        "            return x[:, :3]"
      ],
      "metadata": {
        "id": "bAlu6lX8y9hV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## helpers.py"
      ],
      "metadata": {
        "id": "ACCfrh2izB2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "_logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def load_pretrained(model, cfg=None, num_classes=1000, in_chans=3, filter_fn=None, strict=True):\n",
        "    if cfg is None:\n",
        "        cfg = getattr(model, 'default_cfg')\n",
        "    if cfg is None or 'url' not in cfg or not cfg['url']:\n",
        "        _logger.warning(\"Pretrained model URL is invalid, using random initialization.\")\n",
        "        return\n",
        "\n",
        "    state_dict = model_zoo.load_url(cfg['url'], progress=False, map_location='cpu')\n",
        "\n",
        "    if filter_fn is not None:\n",
        "        state_dict = filter_fn(state_dict)\n",
        "\n",
        "    if in_chans == 1:\n",
        "        conv1_name = cfg['first_conv']\n",
        "        _logger.info('Converting first conv (%s) pretrained weights from 3 to 1 channel' % conv1_name)\n",
        "        conv1_weight = state_dict[conv1_name + '.weight']\n",
        "        # Some weights are in torch.half, ensure it's float for sum on CPU\n",
        "        conv1_type = conv1_weight.dtype\n",
        "        conv1_weight = conv1_weight.float()\n",
        "        O, I, J, K = conv1_weight.shape\n",
        "        if I > 3:\n",
        "            assert conv1_weight.shape[1] % 3 == 0\n",
        "            # For models with space2depth stems\n",
        "            conv1_weight = conv1_weight.reshape(O, I // 3, 3, J, K)\n",
        "            conv1_weight = conv1_weight.sum(dim=2, keepdim=False)\n",
        "        else:\n",
        "            conv1_weight = conv1_weight.sum(dim=1, keepdim=True)\n",
        "        conv1_weight = conv1_weight.to(conv1_type)\n",
        "        state_dict[conv1_name + '.weight'] = conv1_weight\n",
        "    elif in_chans != 3:\n",
        "        conv1_name = cfg['first_conv']\n",
        "        conv1_weight = state_dict[conv1_name + '.weight']\n",
        "        conv1_type = conv1_weight.dtype\n",
        "        conv1_weight = conv1_weight.float()\n",
        "        O, I, J, K = conv1_weight.shape\n",
        "        if I != 3:\n",
        "            _logger.warning('Deleting first conv (%s) from pretrained weights.' % conv1_name)\n",
        "            del state_dict[conv1_name + '.weight']\n",
        "            strict = False\n",
        "        else:\n",
        "            # NOTE this strategy should be better than random init, but there could be other combinations of\n",
        "            # the original RGB input layer weights that'd work better for specific cases.\n",
        "            _logger.info('Repeating first conv (%s) weights in channel dim.' % conv1_name)\n",
        "            repeat = int(math.ceil(in_chans / 3))\n",
        "            conv1_weight = conv1_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]\n",
        "            conv1_weight *= (3 / float(in_chans))\n",
        "            conv1_weight = conv1_weight.to(conv1_type)\n",
        "            state_dict[conv1_name + '.weight'] = conv1_weight\n",
        "\n",
        "    classifier_name = cfg['classifier']\n",
        "    if num_classes == 1000 and cfg['num_classes'] == 1001:\n",
        "        # special case for imagenet trained models with extra background class in pretrained weights\n",
        "        classifier_weight = state_dict[classifier_name + '.weight']\n",
        "        state_dict[classifier_name + '.weight'] = classifier_weight[1:]\n",
        "        classifier_bias = state_dict[classifier_name + '.bias']\n",
        "        state_dict[classifier_name + '.bias'] = classifier_bias[1:]\n",
        "    elif num_classes != cfg['num_classes']:\n",
        "        # completely discard fully connected for all other differences between pretrained and created model\n",
        "        del state_dict[classifier_name + '.weight']\n",
        "        del state_dict[classifier_name + '.bias']\n",
        "        strict = False\n",
        "\n",
        "    model.load_state_dict(state_dict, strict=strict)"
      ],
      "metadata": {
        "id": "q-YnpmY6zDd1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## layers/helpers.py "
      ],
      "metadata": {
        "id": "0usaAgJqzLlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Layer/Module Helpers\n",
        "Hacked together by / Copyright 2020 Ross Wightman\n",
        "\"\"\"\n",
        "from itertools import repeat\n",
        "\n",
        "import collections.abc as container_abcs\n",
        "# from torch._six import container_abcs\n",
        "\n",
        "\n",
        "# From PyTorch internals\n",
        "def _ntuple(n):\n",
        "    def parse(x):\n",
        "        if isinstance(x, container_abcs.Iterable):\n",
        "            return x\n",
        "        return tuple(repeat(x, n))\n",
        "    return parse\n",
        "\n",
        "\n",
        "to_1tuple = _ntuple(1)\n",
        "to_2tuple = _ntuple(2)\n",
        "to_3tuple = _ntuple(3)\n",
        "to_4tuple = _ntuple(4)\n",
        "to_ntuple = _ntuple"
      ],
      "metadata": {
        "id": "obUzIENQzRMJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## layers/drop.py"
      ],
      "metadata": {
        "id": "eVJPwsvpzX-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" DropBlock, DropPath\n",
        "PyTorch implementations of DropBlock and DropPath (Stochastic Depth) regularization layers.\n",
        "Papers:\n",
        "DropBlock: A regularization method for convolutional networks (https://arxiv.org/abs/1810.12890)\n",
        "Deep Networks with Stochastic Depth (https://arxiv.org/abs/1603.09382)\n",
        "Code:\n",
        "DropBlock impl inspired by two Tensorflow impl that I liked:\n",
        " - https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py#L74\n",
        " - https://github.com/clovaai/assembled-cnn/blob/master/nets/blocks.py\n",
        "Hacked together by / Copyright 2020 Ross Wightman\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def drop_block_2d(\n",
        "        x, drop_prob: float = 0.1, block_size: int = 7,  gamma_scale: float = 1.0,\n",
        "        with_noise: bool = False, inplace: bool = False, batchwise: bool = False):\n",
        "    \"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n",
        "    DropBlock with an experimental gaussian noise option. This layer has been tested on a few training\n",
        "    runs with success, but needs further validation and possibly optimization for lower runtime impact.\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    total_size = W * H\n",
        "    clipped_block_size = min(block_size, min(W, H))\n",
        "    # seed_drop_rate, the gamma parameter\n",
        "    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (\n",
        "        (W - block_size + 1) * (H - block_size + 1))\n",
        "\n",
        "    # Forces the block to be inside the feature map.\n",
        "    w_i, h_i = torch.meshgrid(torch.arange(W).to(x.device), torch.arange(H).to(x.device))\n",
        "    valid_block = ((w_i >= clipped_block_size // 2) & (w_i < W - (clipped_block_size - 1) // 2)) & \\\n",
        "                  ((h_i >= clipped_block_size // 2) & (h_i < H - (clipped_block_size - 1) // 2))\n",
        "    valid_block = torch.reshape(valid_block, (1, 1, H, W)).to(dtype=x.dtype)\n",
        "\n",
        "    if batchwise:\n",
        "        # one mask for whole batch, quite a bit faster\n",
        "        uniform_noise = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device)\n",
        "    else:\n",
        "        uniform_noise = torch.rand_like(x)\n",
        "    block_mask = ((2 - gamma - valid_block + uniform_noise) >= 1).to(dtype=x.dtype)\n",
        "    block_mask = -F.max_pool2d(\n",
        "        -block_mask,\n",
        "        kernel_size=clipped_block_size,  # block_size,\n",
        "        stride=1,\n",
        "        padding=clipped_block_size // 2)\n",
        "\n",
        "    if with_noise:\n",
        "        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)\n",
        "        if inplace:\n",
        "            x.mul_(block_mask).add_(normal_noise * (1 - block_mask))\n",
        "        else:\n",
        "            x = x * block_mask + normal_noise * (1 - block_mask)\n",
        "    else:\n",
        "        normalize_scale = (block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-7)).to(x.dtype)\n",
        "        if inplace:\n",
        "            x.mul_(block_mask * normalize_scale)\n",
        "        else:\n",
        "            x = x * block_mask * normalize_scale\n",
        "    return x\n",
        "\n",
        "\n",
        "def drop_block_fast_2d(\n",
        "        x: torch.Tensor, drop_prob: float = 0.1, block_size: int = 7,\n",
        "        gamma_scale: float = 1.0, with_noise: bool = False, inplace: bool = False, batchwise: bool = False):\n",
        "    \"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n",
        "    DropBlock with an experimental gaussian noise option. Simplied from above without concern for valid\n",
        "    block mask at edges.\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    total_size = W * H\n",
        "    clipped_block_size = min(block_size, min(W, H))\n",
        "    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (\n",
        "            (W - block_size + 1) * (H - block_size + 1))\n",
        "\n",
        "    if batchwise:\n",
        "        # one mask for whole batch, quite a bit faster\n",
        "        block_mask = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device) < gamma\n",
        "    else:\n",
        "        # mask per batch element\n",
        "        block_mask = torch.rand_like(x) < gamma\n",
        "    block_mask = F.max_pool2d(\n",
        "        block_mask.to(x.dtype), kernel_size=clipped_block_size, stride=1, padding=clipped_block_size // 2)\n",
        "\n",
        "    if with_noise:\n",
        "        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)\n",
        "        if inplace:\n",
        "            x.mul_(1. - block_mask).add_(normal_noise * block_mask)\n",
        "        else:\n",
        "            x = x * (1. - block_mask) + normal_noise * block_mask\n",
        "    else:\n",
        "        block_mask = 1 - block_mask\n",
        "        normalize_scale = (block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-7)).to(dtype=x.dtype)\n",
        "        if inplace:\n",
        "            x.mul_(block_mask * normalize_scale)\n",
        "        else:\n",
        "            x = x * block_mask * normalize_scale\n",
        "    return x\n",
        "\n",
        "\n",
        "class DropBlock2d(nn.Module):\n",
        "    \"\"\" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 drop_prob=0.1,\n",
        "                 block_size=7,\n",
        "                 gamma_scale=1.0,\n",
        "                 with_noise=False,\n",
        "                 inplace=False,\n",
        "                 batchwise=False,\n",
        "                 fast=True):\n",
        "        super(DropBlock2d, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.gamma_scale = gamma_scale\n",
        "        self.block_size = block_size\n",
        "        self.with_noise = with_noise\n",
        "        self.inplace = inplace\n",
        "        self.batchwise = batchwise\n",
        "        self.fast = fast  # FIXME finish comparisons of fast vs not\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or not self.drop_prob:\n",
        "            return x\n",
        "        if self.fast:\n",
        "            return drop_block_fast_2d(\n",
        "                x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace, self.batchwise)\n",
        "        else:\n",
        "            return drop_block_2d(\n",
        "                x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace, self.batchwise)\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ],
      "metadata": {
        "id": "g_M6GtqKzZ69"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## layers/weight_init.py"
      ],
      "metadata": {
        "id": "Tid-4kGEzee6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
        "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
        "                      \"The distribution of values may be incorrect.\",\n",
        "                      stacklevel=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
        "    normal distribution. The values are effectively drawn from the\n",
        "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
        "    with values outside :math:`[a, b]` redrawn until they are within\n",
        "    the bounds. The method used for generating the random values works\n",
        "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
        "    Args:\n",
        "        tensor: an n-dimensional `torch.Tensor`\n",
        "        mean: the mean of the normal distribution\n",
        "        std: the standard deviation of the normal distribution\n",
        "        a: the minimum cutoff value\n",
        "        b: the maximum cutoff value\n",
        "    Examples:\n",
        "        >>> w = torch.empty(3, 5)\n",
        "        >>> nn.init.trunc_normal_(w)\n",
        "    \"\"\"\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
      ],
      "metadata": {
        "id": "FnbL6apDzgR5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU info"
      ],
      "metadata": {
        "id": "6Vyd0QnOW9gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul9aN5-KW_y4",
        "outputId": "0d75be2a-5076-4754-ad9c-d96f076a289a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-e507df0a-da7a-6312-edb8-c871350fa53d)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main.py"
      ],
      "metadata": {
        "id": "O3P5Z_cKxGwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEqiWTRNwzWF",
        "outputId": "19839b78-9538-47bc-9d57-1affd4b8404c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing dataset cifar..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 30 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(ablate_input=True, ablation_size=4, ablation_target=None, ablation_type='col', additional_hidden=0, adv_eval=None, adv_train=0, arch='deit_tiny_patch16_224', attack_lr=None, attack_steps=None, batch_id=None, batch_size=128, certify=False, certify_ablation_size=4, certify_mode='both', certify_out_dir='OUTDIR_CERT', certify_patch_size=5, certify_stride=1, cifar_preprocess_type='simple224', config_path=None, constraint=None, custom_eps_multiplier=None, custom_lr_multiplier=None, data='/tmp/', data_aug=1, dataset='cifar10', drop_tokens=True, epochs=30, eps=None, eval_only=0, exp_name='model', fff='/root/.local/share/jupyter/runtime/kernel-0626abb0-4c9f-4738-8741-63f5a26a5089.json', freeze_level=-1, log_iters=5, lr=0.01, lr_interpolation='step', mixed_precision=0, model_path=None, momentum=0.9, no_replace_last_layer=False, no_tqdm=0, out_dir='/content/drive/MyDrive/Seminar1', pytorch_pretrained=True, random_restarts=None, random_start=None, resume=False, resume_ckpt_name='checkpoint.pt.latest', resume_optimizer=0, save_ckpt_iters=-1, skip_store=True, step_lr=10, step_lr_gamma=0.1, subset=None, update_BN_stats=False, use_best=None, weight_decay=0.0005, workers=30)\n",
            "==>[Loaded PyTorch-pretrained deit checkpoint.]\n",
            "[Replacing the last layer with 0 hidden layers and 1 classification layer that fits the cifar10 dataset.]\n",
            "==> [Number of parameters of the model is 5526346]\n",
            "{'ablation_size': 4, 'upsample_type': 'simple224', 'return_mask': True, 'do_ablation': True, 'ablation_target': None}\n",
            "Dataset: cifar10 | Model: deit_tiny_patch16_224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING MISSINGNESS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch:0 | Loss 1.7092 | NatPrec1 37.628 | NatPrec5 85.152 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:41<00:00,  9.40it/s]\n",
            "Val Epoch:0 | Loss 1.4399 | NatPrec1 48.330 | NatPrec5 92.080 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:04<00:00, 16.03it/s]\n",
            "Train Epoch:1 | Loss 1.3445 | NatPrec1 52.074 | NatPrec5 92.930 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:40<00:00,  9.75it/s]\n",
            "Train Epoch:2 | Loss 1.2505 | NatPrec1 55.496 | NatPrec5 94.094 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:41<00:00,  9.46it/s]\n",
            "Train Epoch:3 | Loss 1.1865 | NatPrec1 57.616 | NatPrec5 94.872 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.23it/s]\n",
            "Train Epoch:4 | Loss 1.1314 | NatPrec1 59.862 | NatPrec5 95.390 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.28it/s]\n",
            "Train Epoch:5 | Loss 1.1056 | NatPrec1 60.870 | NatPrec5 95.428 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.31it/s]\n",
            "Val Epoch:5 | Loss 1.0886 | NatPrec1 61.480 | NatPrec5 95.780 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:04<00:00, 16.35it/s]\n",
            "Train Epoch:6 | Loss 1.0891 | NatPrec1 61.386 | NatPrec5 95.666 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:43<00:00,  9.03it/s]\n",
            "Train Epoch:7 | Loss 1.0531 | NatPrec1 62.514 | NatPrec5 96.024 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:41<00:00,  9.40it/s]\n",
            "Train Epoch:8 | Loss 1.0482 | NatPrec1 62.818 | NatPrec5 95.924 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.26it/s]\n",
            "Train Epoch:9 | Loss 1.0492 | NatPrec1 62.744 | NatPrec5 96.050 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:41<00:00,  9.33it/s]\n",
            "Train Epoch:10 | Loss 0.9053 | NatPrec1 68.098 | NatPrec5 97.022 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:43<00:00,  9.06it/s]\n",
            "Val Epoch:10 | Loss 0.9112 | NatPrec1 68.130 | NatPrec5 97.070 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:04<00:00, 16.35it/s]\n",
            "Train Epoch:11 | Loss 0.8651 | NatPrec1 69.354 | NatPrec5 97.350 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.29it/s]\n",
            "Train Epoch:12 | Loss 0.8572 | NatPrec1 69.832 | NatPrec5 97.440 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.19it/s]\n",
            "Train Epoch:13 | Loss 0.8522 | NatPrec1 69.884 | NatPrec5 97.436 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:43<00:00,  9.01it/s]\n",
            "Train Epoch:14 | Loss 0.8273 | NatPrec1 70.708 | NatPrec5 97.636 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.28it/s]\n",
            "Train Epoch:15 | Loss 0.8121 | NatPrec1 71.102 | NatPrec5 97.718 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.24it/s]\n",
            "Val Epoch:15 | Loss 0.8831 | NatPrec1 68.850 | NatPrec5 97.230 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:04<00:00, 16.56it/s]\n",
            "Train Epoch:16 | Loss 0.8210 | NatPrec1 70.916 | NatPrec5 97.648 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:43<00:00,  9.07it/s]\n",
            "Train Epoch:17 | Loss 0.8176 | NatPrec1 70.906 | NatPrec5 97.598 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.13it/s]\n",
            "Train Epoch:18 | Loss 0.8177 | NatPrec1 70.894 | NatPrec5 97.580 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:46<00:00,  8.35it/s]\n",
            "Train Epoch:19 | Loss 0.8091 | NatPrec1 71.286 | NatPrec5 97.750 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:43<00:00,  8.90it/s]\n",
            "Train Epoch:20 | Loss 0.7765 | NatPrec1 72.650 | NatPrec5 97.928 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:41<00:00,  9.31it/s]\n",
            "Val Epoch:20 | Loss 0.8659 | NatPrec1 69.960 | NatPrec5 97.170 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:04<00:00, 16.32it/s]\n",
            "Train Epoch:21 | Loss 0.7780 | NatPrec1 72.190 | NatPrec5 97.842 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:43<00:00,  9.08it/s]\n",
            "Train Epoch:22 | Loss 0.7770 | NatPrec1 72.538 | NatPrec5 97.854 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:43<00:00,  9.02it/s]\n",
            "Train Epoch:23 | Loss 0.7687 | NatPrec1 72.422 | NatPrec5 97.958 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.27it/s]\n",
            "Train Epoch:24 | Loss 0.7786 | NatPrec1 72.356 | NatPrec5 97.858 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:41<00:00,  9.38it/s]\n",
            "Train Epoch:25 | Loss 0.7684 | NatPrec1 73.088 | NatPrec5 97.840 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:43<00:00,  9.09it/s]\n",
            "Val Epoch:25 | Loss 0.8533 | NatPrec1 70.060 | NatPrec5 97.540 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:04<00:00, 15.98it/s]\n",
            "Train Epoch:26 | Loss 0.7629 | NatPrec1 73.008 | NatPrec5 97.940 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.27it/s]\n",
            "Train Epoch:27 | Loss 0.7614 | NatPrec1 73.122 | NatPrec5 97.918 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.29it/s]\n",
            "Train Epoch:28 | Loss 0.7596 | NatPrec1 72.990 | NatPrec5 98.026 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:43<00:00,  9.03it/s]\n",
            "Train Epoch:29 | Loss 0.7619 | NatPrec1 73.204 | NatPrec5 97.974 | Reg term: 0.0 ||: 100%|██████████| 391/391 [00:42<00:00,  9.22it/s]\n",
            "Val Epoch:29 | Loss 0.8656 | NatPrec1 69.550 | NatPrec5 97.100 | Reg term: 0.0 ||: 100%|██████████| 79/79 [00:04<00:00, 16.08it/s]\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import cox.store\n",
        "import numpy as np\n",
        "import torch as ch\n",
        "from robustness import datasets, defaults, train\n",
        "\n",
        "if int(os.environ.get(\"NOTEBOOK_MODE\", 0)) == 1:\n",
        "    from tqdm import tqdm_notebook as tqdm\n",
        "else:\n",
        "    from tqdm import tqdm as tqdm\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Transfer learning via pretrained Imagenet models',\n",
        "                                 conflict_handler='resolve')\n",
        "parser = defaults.add_args_to_parser(defaults.CONFIG_ARGS, parser)\n",
        "parser = defaults.add_args_to_parser(defaults.MODEL_LOADER_ARGS, parser)\n",
        "parser = defaults.add_args_to_parser(defaults.TRAINING_ARGS, parser)\n",
        "parser = defaults.add_args_to_parser(defaults.PGD_ARGS, parser)\n",
        "\n",
        "# Custom arguments\n",
        "parser.add_argument('--dataset', type=str, default='cifar10',\n",
        "                    help='Downstream task dataset (Overrides the one in robustness.defaults)')\n",
        "parser.add_argument('--model-path', type=str, help='Path to model trained with robustness lib')\n",
        "parser.add_argument('--resume', action='store_true',\n",
        "                    help='Whether to resume or not (Overrides the one in robustness.defaults)')\n",
        "parser.add_argument('--resume-ckpt-name', type=str, default='checkpoint.pt.latest', \n",
        "                    help='Name of the checkpoint to resume from')\n",
        "parser.add_argument('--pytorch-pretrained', default=True, action='store_true',\n",
        "                    help='If True, loads a Pytorch pretrained model.')\n",
        "parser.add_argument('--subset', type=int, default=None,\n",
        "                    help='number of training data to use from the dataset')\n",
        "parser.add_argument('--no-tqdm', type=int, default=0,\n",
        "                    choices=[0, 1], help='Do not use tqdm.')\n",
        "parser.add_argument('--no-replace-last-layer', action='store_true',\n",
        "                    help='Whether to avoid replacing the last layer')\n",
        "parser.add_argument('--freeze-level', type=int, default=-1,\n",
        "                    help='Up to what layer to freeze in the pretrained model (assumes a resnet architectures)')\n",
        "parser.add_argument('--additional-hidden', type=int, default=0,\n",
        "                    help='How many hidden layers to add on top of pretrained network + classification layer')\n",
        "parser.add_argument('--update-BN-stats', action='store_true')\n",
        "parser.add_argument('--cifar-preprocess-type', type=str, default='simple224', \n",
        "                    help='What cifar preprocess type to use (only use with CIFAR-10)',\n",
        "                    choices=['simple224', 'upsample384', 'none'])\n",
        "parser.add_argument('--drop-tokens', default=True, action='store_true', help='drop masked out tokens (for ViTs only)')\n",
        "parser.add_argument('--ablation-target', type=int, default=None, help='choose specific column to keep')\n",
        "\n",
        "\n",
        "## Input Ablation\n",
        "parser.add_argument('--ablate-input', '--ablate_input', default=True, action='store_true')\n",
        "parser.add_argument('--ablation-type', '--ablation_type', type=str, default='col',\n",
        "                    help='Type of ablations', choices=['col', 'block'])\n",
        "parser.add_argument('--ablation-size', '--ablation_size', type=int, default=4,\n",
        "                    help='Width of the remaining column if --ablation-type is \"col\".' \n",
        "                    'Side length of the remaining block if --ablation-type is \"block\"')\n",
        "\n",
        "# certification arguments\n",
        "parser.add_argument('--skip-store', default=True, action='store_true')\n",
        "parser.add_argument('--certify', action='store_true')\n",
        "parser.add_argument('--certify-out-dir', default='OUTDIR_CERT')\n",
        "parser.add_argument('--certify-mode', default='both', choices=['both', 'row', 'col', 'block'])\n",
        "parser.add_argument('--certify-ablation-size', type=int, default=4)\n",
        "parser.add_argument('--certify-patch-size', type=int, default=5)\n",
        "parser.add_argument('--certify-stride', type=int, default=1)\n",
        "parser.add_argument('--batch-id', type=int, default=None)\n",
        "\n",
        "# Colab Adjustments\n",
        "parser.add_argument('--out-dir', '--out_dir', default='/content/drive/MyDrive/Seminar1')\n",
        "parser.add_argument('--adv-train', '--adv_train', default=0)\n",
        "parser.add_argument('--arch', default='deit_tiny_patch16_224')\n",
        "parser.add_argument('--exp-name', '--exp_name', default='model')\n",
        "parser.add_argument('-f', '--fff', help='a dummy argument to fool ipython', default='1')\n",
        "\n",
        "def main(args, store):\n",
        "    '''Given arguments and a cox store, trains as a model. Check out the \n",
        "    argparse object in this file for argument options.\n",
        "    '''\n",
        "    ds, train_loader, validation_loader = get_dataset_and_loaders(args)\n",
        "    print(args)\n",
        "    model, checkpoint = get_model(args, ds)\n",
        "\n",
        "    def get_n_params(model):\n",
        "        total = 0\n",
        "        for p in list(model.parameters()):\n",
        "            total += np.prod(p.size())\n",
        "        return total\n",
        "    print(f'==> [Number of parameters of the model is {get_n_params(model)}]')\n",
        "    \n",
        "    model.normalizer = PreProcessor(\n",
        "        normalizer=model.normalizer,\n",
        "        ablation_size=args.ablation_size, \n",
        "        upsample_type=args.cifar_preprocess_type,\n",
        "        return_mask=args.drop_tokens,\n",
        "        do_ablation=args.ablate_input,\n",
        "        ablation_type=args.ablation_type,\n",
        "        ablation_target=args.ablation_target,\n",
        "    )\n",
        "    if 'deit' not in args.arch:\n",
        "        assert args.drop_tokens == False\n",
        "    \n",
        "    if args.update_BN_stats:\n",
        "        print(f'==>[Started updating the BN stats relevant to {args.dataset}]')\n",
        "        assert not hasattr(model, \"module\"), \"model is already in DataParallel.\"\n",
        "        model = model.cuda()\n",
        "        model.train()\n",
        "        attack_kwargs = {\n",
        "            'constraint': args.constraint,\n",
        "            'eps': args.eps,\n",
        "            'step_size': args.attack_lr,\n",
        "            'iterations': args.attack_steps,\n",
        "        }\n",
        "        with ch.no_grad():\n",
        "            niters = 0\n",
        "            while niters < 200:\n",
        "                iterator = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "                for _, (inp, _) in iterator:\n",
        "                    model(inp.cuda(), **attack_kwargs)\n",
        "                    niters += 1\n",
        "        print('==>[Updated the BN stats]')\n",
        "\n",
        "    if args.eval_only:\n",
        "        if args.certify: \n",
        "            return certify(args, model, validation_loader, store=store)\n",
        "        else: \n",
        "            return train.eval_model(args, model, validation_loader, store=store)\n",
        "\n",
        "    update_params = freeze_model(model, freeze_level=args.freeze_level)\n",
        "    print(f\"Dataset: {args.dataset} | Model: {args.arch}\")\n",
        "\n",
        "    train.train_model(args, model, (train_loader, validation_loader), store=store,\n",
        "                        checkpoint=checkpoint, update_params=update_params)\n",
        "\n",
        "\n",
        "def args_preprocess(args):\n",
        "    '''\n",
        "    Fill the args object with reasonable defaults, and also perform a sanity check to make sure no\n",
        "    args are missing.\n",
        "    '''\n",
        "    if args.adv_train and eval(args.eps) == 0:\n",
        "        print('[Switching to standard training since eps = 0]')\n",
        "        args.adv_train = 0\n",
        "\n",
        "    if args.pytorch_pretrained:\n",
        "        assert not args.model_path, 'You can either specify pytorch_pretrained or model_path, not together.'\n",
        "\n",
        "\n",
        "    ALL_DS = TRANSFER_DATASETS + ['imagenet', 'stylized_imagenet']\n",
        "\n",
        "\n",
        "    #Prepare training args\n",
        "    # Preprocess args\n",
        "    default_ds = args.dataset if args.dataset in datasets.DATASETS else \"cifar\"\n",
        "    args = defaults.check_and_fill_args(args, defaults.CONFIG_ARGS, datasets.CIFAR)\n",
        "    if not args.eval_only:\n",
        "        args = defaults.check_and_fill_args(args, defaults.TRAINING_ARGS, datasets.CIFAR)\n",
        "    if args.adv_train or args.adv_eval:\n",
        "        args = defaults.check_and_fill_args(args, defaults.PGD_ARGS, datasets.CIFAR)\n",
        "    args = defaults.check_and_fill_args(args, defaults.MODEL_LOADER_ARGS, datasets.CIFAR)\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "args = parser.parse_args()\n",
        "args = args_preprocess(args)\n",
        "if args.skip_store: \n",
        "    store = None\n",
        "\n",
        "main(args, store)"
      ]
    }
  ]
}